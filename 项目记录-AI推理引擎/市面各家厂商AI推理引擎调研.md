
目前，AI推理引擎大部分都只能在特定硬件上使用，对于同时使用不同品牌硬件的企业来说，就需要针对不同的硬件来处理软件层。

和训练框架一样，推理框架同样生态繁多，但当你接触过的框架较多之后，就应该能够发现他们之间的一些共同点：推理流程都是一致的。我画了下面这个图来进行展示和说明，这里的每一个模块的重要性都很高。拿前端解析来说就很难，因为训练框架太多了，同一个模型可能会用不同的框架进行训练，那就需要支持从不同的框架往 ONNX 或者往你自定义的模型结构进行转换和收敛，这个过程是复杂的。这个复杂性一方面来自于不同框架之间的区别，算子粒度、算子描述、算子参数等可能都会有所区别；另一方面来自于 ONNX 本身，ONNX 本身具有一些问题，比如 ONNX opset 版本之间的不统一，比如胶水算子比较多导致对部署不友好等。接着是模型优化，模型优化是推理引擎中最重要的模块，里面的内容也为最多。对于模型优化，我觉得大致可以分为上层优化和底层优化两个部分。上层优化或者又可称为图优化、IR 优化，包括像耳熟能详的量化、算子融合、部署不友好算子的替换等都属于这个层级；而底层优化会更加多的涉及软硬件协同的优化，比如会借助指令集的加速做一些向量化的实现，比如会更多考虑访存方面的问题来调节算子的调度，比如采用循环拆分来更好地使用并行计算等。正是由于模型优化这一块过于复杂，过于博大精深，所以衍生出了编译框架来对专门对这一块进行优化，对于编译框架后面我会专门写文章进行讨论。一个推理框架的性能怎么样，往往在于它模型优化这一块做的怎么样，如果模型优化这一块做的足够好，那么整个推理框架的性能也会比较突出。在完成了前端模型解析和模型优化后，会进入到常规的模型推理流程，即 前处理 -> 模型推理 -> 后处理。在实际业务中，对于推理框架来说，并不会在前处理和后处理中做太丰富的支持，前处理和后处理模块大多会由用户来主导，比如会使用一些如 OpenCV、npp 之类的通用库来进行处理。一些急于推广的推理框架，为了降低适配难度以及提高整体 pipeline 的性能，另有些情况是需要自己实现一些常用算子以适配自家的芯片，这样可能会在自己的推理引擎中以自定义算子 plugin 的形式集成一些前后处理的典型算子，比如 yolo、比如 resize。
![image.png](https://raw.githubusercontent.com/KrealHtz/NoteImage/master/data/202405081501652.png)
推理框架按使用场景、使用类型来说，大致可以分为两类，一类是自用的，一类是通用的。显然，自用的大多其实是芯片厂商的推理框架，为了让自家的芯片可用，所以推出了适配自家芯片的推理框架。自用的推理框架在市场上的占用率应该是占主导的，其中为代表的肯定还是 TensorRT，另外还有像英特尔的 OpenVino 以及我国各国产芯片厂商的 sdk。自用推理框架的特点十分明显，即在自家的芯片上适配的很好而且效率很高，但不同芯片之间不能通用。自用推理框架的这个毛病让用户很是痛苦，拿我来说，我至少接触和开发过 TensorRT、Openvino、昇腾的推理框架、寒武纪的推理框架、算能的推理框架、登临的推理框架、曙光的推理框架、昆仑的推理框架、海思的推理框架、全志的推理框架等，很多时候都会觉得麻烦。而通用的推理框架就是为了解决这个痛点，通用的推理框架中，NCNN 主要还是面向移动端，覆盖的通用范畴并不够算广；而 Tengine 则明显后劲不足，推理框架大一统方面可能最有希望的还是如 TVM 的编译框架了。所以从某种角度来说，对于 TVM 的期望，其实跟对 ONNX 的期望是一样的。
![image.png](https://raw.githubusercontent.com/KrealHtz/NoteImage/master/data/202405081501300.png)
