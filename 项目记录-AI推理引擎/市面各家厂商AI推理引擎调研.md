
目前，AI推理引擎大部分都只能在特定硬件上使用，对于同时使用不同品牌硬件的企业来说，就需要针对不同的硬件来处理软件层

和训练框架一样，推理框架同样生态繁多，但当你接触过的框架较多之后，就应该能够发现他们之间的一些共同点：推理流程都是一致的。我画了下面这个图来进行展示和说明，这里的每一个模块的重要性都很高。拿前端解析来说就很难，因为训练框架太多了，同一个模型可能会用不同的框架进行训练，那就需要支持从不同的框架往 ONNX 或者往你自定义的模型结构进行转换和收敛，这个过程是复杂的。这个复杂性一方面来自于不同框架之间的区别，算子粒度、算子描述、算子参数等可能都会有所区别；另一方面来自于 ONNX 本身，ONNX 本身具有一些问题，比如 ONNX opset 版本之间的不统一，比如胶水算子比较多导致对部署不友好等。接着是模型优化，模型优化是推理引擎中最重要的模块，里面的内容也为最多。对于模型优化，我觉得大致可以分为上层优化和底层优化两个部分。上层优化或者又可称为图优化、IR 优化，包括像耳熟能详的量化、算子融合、部署不友好算子的替换等都属于这个层级；而底层优化会更加多的涉及软硬件协同的优化，比如会借助指令集的加速做一些向量化的实现，比如会更多考虑访存方面的问题来调节算子的调度，比如采用循环拆分来更好地使用并行计算等。正是由于模型优化这一块过于复杂，过于博大精深，所以衍生出了编译框架来对专门对这一块进行优化，对于编译框架后面我会专门写文章进行讨论。一个推理框架的性能怎么样，往往在于它模型优化这一块做的怎么样，如果模型优化这一块做的足够好，那么整个推理框架的性能也会比较突出。在完成了前端模型解析和模型优化后，会进入到常规的模型推理流程，即 前处理 -> 模型推理 -> 后处理。在实际业务中，对于推理框架来说，并不会在前处理和后处理中做太丰富的支持，前处理和后处理模块大多会由用户来主导，比如会使用一些如 OpenCV、npp 之类的通用库来进行处理。一些急于推广的推理框架，为了降低适配难度以及提高整体 pipeline 的性能，另有些情况是需要自己实现一些常用算子以适配自家的芯片，这样可能会在自己的推理引擎中以自定义算子 plugin 的形式集成一些前后处理的典型算子，比如 yolo、比如 resize。